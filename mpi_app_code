# -*- coding: utf-8 -*-
"""mpi_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c-FGVjV1fh6tqJyEAp3Fs-kFwE9IMFK7
"""





import streamlit as st
import pandas as pd
import numpy as np
import cv2
from PIL import Image
from mpi4py import MPI
from sklearn.linear_model import LinearRegression
import io
import matplotlib.pyplot as plt
import statistics

# Initialize MPI communication
comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# Odd-Even Transposition Sort (used in parallel sorting)
def odd_even_sort(data):
    n = len(data)
    for i in range(n):
        for j in range(i % 2, n - 1, 2):
            if data[j] > data[j + 1]:
                data[j], data[j + 1] = data[j + 1], data[j]
    return data

# Task 1: Parallel Sorting
def parallel_sort():
    # Initialize chunks for all ranks
    chunks = None
    if rank == 0:
        # Get user input from Streamlit
        nums = st.text_input("Enter numbers separated by commas:")
        if nums:
            numbers = list(map(int, nums.split(',')))
            chunks = np.array_split(numbers, size)
        # If nums is empty, chunks remains None, which is handled by scatter

    # Scatter chunks to different processes
    # scatter handles the case where the root provides None if no data is uploaded
    chunk = comm.scatter(chunks, root=0)

    # Process the chunk only if it's not None (relevant if no data was uploaded)
    if chunk is not None:
        sorted_chunk = odd_even_sort(list(chunk))
        gathered = comm.gather(sorted_chunk, root=0)

        if rank == 0:
            # Gather and finalize sorting on root
            # Ensure gathered is not None before processing
            if gathered is not None:
                result = sum(gathered, [])
                st.write("Sorted Result:", odd_even_sort(result))

# Task 2: File Processing (.txt file)
def file_processing():
    if rank == 0:
        uploaded_file = st.file_uploader("Upload a .txt file", type="txt")
        if uploaded_file:
            text = uploaded_file.read().decode("utf-8")
            words = text.split()
            chunks = np.array_split(words, size)
        else:
            return
    else:
        chunks = None

    # Distribute word chunks
    chunk = comm.scatter(chunks, root=0)
    local_words = list(set(chunk))
    local_count = len(chunk)
    gathered_words = comm.gather(local_words, root=0)
    gathered_counts = comm.gather(local_count, root=0)

    if rank == 0:
        # Combine and display word statistics
        all_words = sum(gathered_words, [])
        unique_words = set(all_words)
        total_words = sum(gathered_counts)
        st.write(f"Total words: {total_words}")
        st.write(f"Unique words: {len(unique_words)}")

# Task 3: Image Processing (Grayscale or Blur)
def image_processing():
    if rank == 0:
        img_file = st.file_uploader("Upload an image", type=['jpg', 'png'])
        if img_file:
            image = Image.open(img_file)
            img_array = np.array(image)
            st.image(image, caption='Original Image', use_column_width=True)
            filter_type = st.selectbox("Choose filter", ["Grayscale", "Blur"])

            # Apply selected image filter
            if filter_type == "Grayscale":
                processed = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            elif filter_type == "Blur":
                processed = cv2.GaussianBlur(img_array, (7, 7), 0)

            # Show processed image
            st.image(processed, caption='Processed Image', use_column_width=True)

# Task 4: Machine Learning (Linear Regression)
def ml_training():
    if rank == 0:
        uploaded_file = st.file_uploader("Upload CSV for Linear Regression", type='csv')
        if uploaded_file:
            df = pd.read_csv(uploaded_file)
            X = df.iloc[:, :-1].values  # Features
            y = df.iloc[:, -1].values   # Target
            chunks = np.array_split(X, size)
            y_chunks = np.array_split(y, size)
        else:
            return
    else:
        chunks = None
        y_chunks = None

    # Distribute training data
    X_local = comm.scatter(chunks, root=0)
    y_local = comm.scatter(y_chunks, root=0)

    # Train model locally
    model = LinearRegression().fit(X_local, y_local)
    coef = model.coef_
    intercept = model.intercept_
    gathered = comm.gather((coef, intercept), root=0)

    if rank == 0:
        # Average the coefficients and intercepts
        avg_coef = np.mean([g[0] for g in gathered], axis=0)
        avg_intercept = np.mean([g[1] for g in gathered])
        st.write("Average Coefficients:", avg_coef)
        st.write("Average Intercept:", avg_intercept)

# Task 5: Parallel Keyword Search in Text File
def parallel_search():
    if rank == 0:
        file = st.file_uploader("Upload a .txt file for keyword search", type='txt')
        keyword = st.text_input("Enter keyword to search:")
        if file and keyword:
            text = file.read().decode("utf-8")
            lines = text.splitlines()
            chunks = np.array_split(lines, size)
        else:
            return
    else:
        chunks = None
        keyword = None

    lines = comm.scatter(chunks, root=0)
    keyword = comm.bcast(keyword, root=0)
    positions = []
    total = 0

    for i, line in enumerate(lines):
        if keyword in line:
            total += line.count(keyword)
            positions.append(i)

    gathered_results = comm.gather((total, positions), root=0)

    if rank == 0:
        final_count = sum([res[0] for res in gathered_results])
        all_positions = sum([res[1] for res in gathered_results], [])
        st.write(f"Total occurrences: {final_count}")
        st.write(f"Line positions: {all_positions}")

# Task 6: Parallel Statistics Analyzer
def parallel_statistics():
    if rank == 0:
        file = st.file_uploader("Upload a CSV file for statistical analysis", type='csv')
        if file:
            df = pd.read_csv(file)
            chunks = np.array_split(df, size)
        else:
            return
    else:
        chunks = None

    local_df = comm.scatter(chunks, root=0)
    local_stats = {}

    for column in local_df.select_dtypes(include=np.number).columns:
        col_data = local_df[column].dropna().tolist()
        if col_data:
            local_stats[column] = {
                'mean': np.mean(col_data),
                'median': np.median(col_data),
                'mode': statistics.mode(col_data),
                'min': np.min(col_data),
                'max': np.max(col_data),
                'std': np.std(col_data)
            }

    gathered = comm.gather(local_stats, root=0)

    if rank == 0:
        final_stats = {}
        for stat in gathered:
            for k, v in stat.items():
                if k not in final_stats:
                    final_stats[k] = {m: [] for m in v}
                for m in v:
                    final_stats[k][m].append(v[m])

        for k in final_stats:
            st.write(f"Statistics for column: {k}")
            for m in final_stats[k]:
                st.write(f"{m}: {np.mean(final_stats[k][m])}")

# Task 7: Distributed Matrix Multiplication
def matrix_multiplication():
    if rank == 0:
        file1 = st.file_uploader("Upload first matrix CSV", key="mat1")
        file2 = st.file_uploader("Upload second matrix CSV", key="mat2")
        if file1 and file2:
            A = pd.read_csv(file1, header=None).values
            B = pd.read_csv(file2, header=None).values
            if A.shape[1] != B.shape[0]:
                st.error("Incompatible matrix dimensions for multiplication")
                return
            chunks = np.array_split(A, size)
        else:
            return
    else:
        B = None
        chunks = None

    local_A = comm.scatter(chunks, root=0)
    B = comm.bcast(B, root=0)

    local_result = np.dot(local_A, B)
    gathered = comm.gather(local_result, root=0)

    if rank == 0:
        final_matrix = np.vstack(gathered)
        st.write("Resultant Matrix:")
        st.dataframe(final_matrix)

# Main function to run the Streamlit app

def run_app():
    if rank == 0:
        st.title("Parallel Data Processing with MPI")
        task = st.sidebar.selectbox("Choose a task", [
            "Sorting", "File Processing", "Image Processing", "ML Training",
            "Parallel Search", "Statistics Analyzer", "Matrix Multiplication"
        ])

        if task == "Sorting":
            parallel_sort()
        elif task == "File Processing":
            file_processing()
        elif task == "Image Processing":
            image_processing()
        elif task == "ML Training":
            ml_training()
        elif task == "Parallel Search":
            parallel_search()
        elif task == "Statistics Analyzer":
            parallel_statistics()
        elif task == "Matrix Multiplication":
            matrix_multiplication()

# Entry point for the script
if __name__ == '__main__':
    run_app()
